{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import warnings\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "more_stop_words = ['это', 'также', 'данный', 'слово', 'который']\n",
    "for word in more_stop_words:\n",
    "    russian_stopwords.append(word)\n",
    "\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "# Preprocess function\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\n",
    "              and token != \" \"\n",
    "              and token.strip() not in punctuation]\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reports datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_answered = pd.read_csv('Reports-examples.csv', error_bad_lines=False)\n",
    "# dataset_answered = dataset_answered.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_answers = pd.read_csv('Reports-no-answers.csv', error_bad_lines=False)\n",
    "dataset_no_answers.drop([' spare column ', '№ обращения'],inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_df = dataset_no_answers \n",
    "# print(dataset_no_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_answered\n",
    "# dataset_no_answers\n",
    "temp_df2 = temp_df\n",
    "i = 0\n",
    "for report in temp_df[' текст_обращения']:    \n",
    "    # print(preprocess_text(report), '\\n')\n",
    "    temp_df2[' текст_обращения'][i] = preprocess_text(report)\n",
    "    i += 1\n",
    "temp_df2.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df2.to_csv('clean_no_answered_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get clear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_answers = pd.read_csv('clean_no_answered_dataset.csv')\n",
    "# dataset_no_answers = temp_df2[' текст_обращения'].values\n",
    "dataset_no_answers = dataset_no_answers.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "dataset_no_answers = dataset_no_answers[' текст_обращения'].values\n",
    "\n",
    "# dataset_no_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 250\n",
    "no_features = 70\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features, min_df=1, stop_words=russian_stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset_no_answers)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# tfidf_feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_features=no_features, stop_words=russian_stopwords)\n",
    "tf = tf_vectorizer.fit_transform(dataset_no_answers)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "# tf_feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.0, random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF {'приходить', 'предоставлять', 'сдача', 'мочь', 'ждать', 'лаборатория', 'обращаться', 'свой', 'линия', 'забор', 'очень', 'пз', 'просить', 'прием', 'требовать', 'администрация', 'время', 'горячий', 'анализ', 'обращение', 'пцр', '30', 'заказ', 'запись', 'документ', 'почта', 'врач', 'недовольный', 'код', 'сотрудник', 'человек', 'жалоба', 'кровь', 'пациент', 'связь', 'считать', 'обратный', 'сказать', 'работа', 'итог', 'день', 'работать', 'сдавать', 'очередь', 'отказывать', 'результат', 'скидка', 'принимать', 'звонок', 'медсестра', 'исследование', 'претензия', 'филиал', 'сестра', 'отказываться', 'коронавирус', 'смочь', 'регистратор', 'пациентка', 'мера', 'разбираться', 'мед', 'медицинский', 'указывать', 'хотеть', 'клиент', 'антитело', 'взять', 'выполнять', 'ребенок'}\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    result_set = set()  # множество всех слов из всех сгенерированных тем\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        s = \"Topic %d: \" % (topic_idx)\n",
    "        s1 = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        for word in s1:\n",
    "            result_set.add(word)\n",
    "            s += word\n",
    "            s += ' '\n",
    "        # print(s)\n",
    "    return result_set\n",
    "\n",
    "\n",
    "no_top_words = 40\n",
    "print('NMF', display_topics(nmf, tfidf_feature_names, no_top_words))\n",
    "# print('LDA', display_topics(lda, tf_feature_names, no_top_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorythms ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "администрация филиал претензия ждать средство претензия мед качество маска суть работник обслуживание перчатка звонок индивидуальный направлять пациент отмечать улучшение филиал находиться защита \n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (2, 1), indices imply (2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\PythonProgs\\Medicine\\epsilon-six-nine\\nlp.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProgs/Medicine/epsilon-six-nine/nlp.ipynb#ch0000031?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(new_shuffled_string)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProgs/Medicine/epsilon-six-nine/nlp.ipynb#ch0000031?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(label)   \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PythonProgs/Medicine/epsilon-six-nine/nlp.ipynb#ch0000031?line=22'>23</a>\u001b[0m df2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame([new_shuffled_string, label], columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m текст_обращения\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProgs/Medicine/epsilon-six-nine/nlp.ipynb#ch0000031?line=24'>25</a>\u001b[0m augmented_dataset\u001b[39m.\u001b[39mappend(df2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PythonProgs/Medicine/epsilon-six-nine/nlp.ipynb#ch0000031?line=25'>26</a>\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mD:\\PythonProgs\\BaseVenv\\NewEnv\\lib\\site-packages\\pandas\\core\\frame.py:737\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=728'>729</a>\u001b[0m         mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=729'>730</a>\u001b[0m             arrays,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=730'>731</a>\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=733'>734</a>\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=734'>735</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=735'>736</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=736'>737</a>\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=737'>738</a>\u001b[0m             data,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=738'>739</a>\u001b[0m             index,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=739'>740</a>\u001b[0m             columns,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=740'>741</a>\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=741'>742</a>\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=742'>743</a>\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=743'>744</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=744'>745</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=745'>746</a>\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=746'>747</a>\u001b[0m         {},\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=747'>748</a>\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=750'>751</a>\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/frame.py?line=751'>752</a>\u001b[0m     )\n",
      "File \u001b[1;32mD:\\PythonProgs\\BaseVenv\\NewEnv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=345'>346</a>\u001b[0m \u001b[39m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=346'>347</a>\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=347'>348</a>\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=348'>349</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=350'>351</a>\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=352'>353</a>\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=354'>355</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mD:\\PythonProgs\\BaseVenv\\NewEnv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:422\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=419'>420</a>\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=420'>421</a>\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> <a href='file:///d%3A/PythonProgs/BaseVenv/NewEnv/lib/site-packages/pandas/core/internals/construction.py?line=421'>422</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (2, 1), indices imply (2, 2)"
     ]
    }
   ],
   "source": [
    "# augmentation\n",
    "import random\n",
    "dataset_no_answers = pd.read_csv('dataset-3-labels.csv')\n",
    "augmented_dataset = dataset_no_answers\n",
    "i = 0\n",
    "for item in dataset_no_answers[' текст_обращения']:\n",
    "    shuffled_array = item.split()\n",
    "    shuffled_array = random.sample(shuffled_array, len(shuffled_array))\n",
    "    label = dataset_no_answers['label'][i]\n",
    "\n",
    "\n",
    "    # print(shuffled_array)\n",
    "    to_append = {' текст_обращения':shuffled_array, 'label':label}\n",
    "    # print(to_append['label'])\n",
    "\n",
    "    # все для вставки ок, но не вставляется\n",
    "    # augmented_dataset.append(to_append, ignore_index=True)\n",
    "    # augmented_dataset.append(shuffled_array, label)\n",
    "\n",
    "    new_shuffled_string = ''.join(str(e)+' ' for e in shuffled_array) \n",
    "    print(new_shuffled_string)\n",
    "    print(label)   \n",
    "    df2 = pd.DataFrame([new_shuffled_string, label], columns=[' текст_обращения', 'label'])\n",
    "    \n",
    "    augmented_dataset.append(df2)\n",
    "    i += 1\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.44151733, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.31753619, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.20567761, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.11894921, 0.25750298,\n",
       "        0.25750298],\n",
       "       [0.        , 0.        , 0.        , ..., 0.08998565, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "# dataset_no_answers = pd.read_csv('nn-labeled-dataset.csv') # 4 labels\n",
    "dataset_no_answers = pd.read_csv('dataset-3-labels.csv') # 3 labels\n",
    "# dataset_no_answers = dataset_no_answers.drop('Unnamed: 0', axis=1) # только для 4 меток, там лишняя колонка\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features, min_df=1, stop_words=russian_stopwords)\n",
    "X = tfidf_vectorizer.fit_transform(dataset_no_answers[' текст_обращения']).toarray()\n",
    "y = dataset_no_answers['label']\n",
    "\n",
    "X\n",
    "# dataset_no_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43478260869565216"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=5000, random_state=0, max_depth=1000) # 56%\n",
    "clf = GradientBoostingClassifier(n_estimators=10000, learning_rate=0.01, max_depth=100, random_state=0)\n",
    "# clf = make_pipeline(StandardScaler(), SVC(kernel='sigmoid', gamma='scale'))\n",
    "# clf = MultinomialNB() # 65% best\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 0, 1, 0, 1, 0, 3, 1, 1, 3, 3, 0, 1, 0, 1, 0, 3, 3, 1, 0, 0,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1 2]\n",
      " [4 3 1]\n",
      " [2 3 3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.57      0.47         7\n",
      "           1       0.43      0.38      0.40         8\n",
      "           3       0.50      0.38      0.43         8\n",
      "\n",
      "    accuracy                           0.43        23\n",
      "   macro avg       0.44      0.44      0.43        23\n",
      "weighted avg       0.44      0.43      0.43        23\n",
      "\n",
      "0.43478260869565216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bf4ecafce57d7e80fd4fa0549b97616699ad25c032d6958759ccc36dc147537"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('NewEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
