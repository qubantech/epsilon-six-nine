{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import warnings\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "more_stop_words = ['это', 'также', 'данный', 'слово', 'который']\n",
    "for word in more_stop_words:\n",
    "    russian_stopwords.append(word)\n",
    "russian_stopwords\n",
    "# with open('assets/stopwords.csv') as f:\n",
    "#     f.write(str(russian_stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "# Preprocess function\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\n",
    "              and token != \" \"\n",
    "              and token.strip() not in punctuation]\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get reports datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_answered = pd.read_csv('Reports-examples.csv', error_bad_lines=False)\n",
    "# dataset_answered = dataset_answered.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_answers = pd.read_csv('Reports-no-answers.csv', error_bad_lines=False)\n",
    "dataset_no_answers.drop([' spare column ', '№ обращения'],inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_df = dataset_no_answers \n",
    "# print(dataset_no_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_answered\n",
    "# dataset_no_answers\n",
    "temp_df2 = temp_df\n",
    "i = 0\n",
    "for report in temp_df[' текст_обращения']:    \n",
    "    # print(preprocess_text(report), '\\n')\n",
    "    temp_df2[' текст_обращения'][i] = preprocess_text(report)\n",
    "    i += 1\n",
    "temp_df2.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df2.to_csv('clean_no_answered_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get clear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_answers = pd.read_csv('clean_no_answered_dataset.csv')\n",
    "# dataset_no_answers = temp_df2[' текст_обращения'].values\n",
    "dataset_no_answers = dataset_no_answers.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "dataset_no_answers = dataset_no_answers[' текст_обращения'].values\n",
    "\n",
    "# dataset_no_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 250\n",
    "# no_features = 70\n",
    "no_features = 21\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features, min_df=1, stop_words=russian_stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset_no_answers)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "# tfidf_feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_features=no_features, stop_words=russian_stopwords)\n",
    "tf = tf_vectorizer.fit_transform(dataset_no_answers)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "# tf_feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.0, random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF {'приходить', 'предоставлять', 'сдача', 'мочь', 'ждать', 'лаборатория', 'обращаться', 'свой', 'линия', 'забор', 'очень', 'пз', 'просить', 'прием', 'требовать', 'администрация', 'время', 'горячий', 'анализ', 'обращение', 'пцр', '30', 'заказ', 'запись', 'документ', 'почта', 'врач', 'недовольный', 'код', 'сотрудник', 'человек', 'жалоба', 'кровь', 'пациент', 'связь', 'считать', 'обратный', 'сказать', 'работа', 'итог', 'день', 'работать', 'сдавать', 'очередь', 'отказывать', 'результат', 'скидка', 'принимать', 'звонок', 'медсестра', 'исследование', 'претензия', 'филиал', 'сестра', 'отказываться', 'коронавирус', 'смочь', 'регистратор', 'пациентка', 'мера', 'разбираться', 'мед', 'медицинский', 'указывать', 'хотеть', 'клиент', 'антитело', 'взять', 'выполнять', 'ребенок'}\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    result_set = set()  # множество всех слов из всех сгенерированных тем\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        s = \"Topic %d: \" % (topic_idx)\n",
    "        s1 = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        for word in s1:\n",
    "            result_set.add(word)\n",
    "            s += word\n",
    "            s += ' '\n",
    "        # print(s)\n",
    "    return result_set\n",
    "\n",
    "\n",
    "no_top_words = 40\n",
    "print('NMF', display_topics(nmf, tfidf_feature_names, no_top_words))\n",
    "# print('LDA', display_topics(lda, tf_feature_names, no_top_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorythms ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation\n",
    "import random\n",
    "dataset_no_answers = pd.read_csv('dataset-3-labels.csv')\n",
    "dataset_no_answers.to_csv('augmented-dataset.csv')\n",
    "\n",
    "augmented_dataset = dataset_no_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(dataset_no_answers['label'].array)):\n",
    "\n",
    "    unshuffled_array = dataset_no_answers[' текст_обращения'][i]\n",
    "    label = dataset_no_answers['label'][i]\n",
    "    shuffled_array = unshuffled_array.split()\n",
    "    shuffled_array = random.sample(shuffled_array, len(shuffled_array))\n",
    "    l = len(dataset_no_answers[' текст_обращения'])\n",
    "    \n",
    "    \n",
    "\n",
    "    # print(shuffled_array)\n",
    "    # to_append = {' текст_обращения':shuffled_array, 'label':label}\n",
    "    # print(to_append['label'])\n",
    "\n",
    "    # все для вставки ок, но не вставляется\n",
    "    # augmented_dataset.append(to_append, ignore_index=True)\n",
    "    # augmented_dataset.append(shuffled_array, label)\n",
    "\n",
    "    new_shuffled_string = ''.join(str(e)+' ' for e in shuffled_array) \n",
    "    # print(new_shuffled_string)\n",
    "    # print(label)   \n",
    "\n",
    "    with open('augmented-dataset.csv', 'a', encoding = 'utf-8') as file:\n",
    "        file.write(''.join(\"\\n\"))\n",
    "        file.write(str(l + i))\n",
    "        file.write(',')\n",
    "        file.write(new_shuffled_string)\n",
    "        file.write(',')\n",
    "        file.write(str(label))\n",
    "    # df2 = pd.DataFrame([[new_shuffled_string, label]], columns=[' текст_обращения', 'label'])\n",
    "    # augmented_dataset.append(df2)\n",
    "    i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>текст_обращения</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>суть претензия мед работник находиться филиал ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>доводить ваш сведение горячий линия обращаться...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>доводить ваш сведение горячий линия обращаться...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>доводить ваш сведение горячий линия обращаться...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>доводить ваш сведение горячий линия обращаться...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>294</td>\n",
       "      <td>год приходиться сегодня ребенок очередь обраща...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>295</td>\n",
       "      <td>пациентка предъявление полагать претензия родс...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>296</td>\n",
       "      <td>пациентка анализ анализ герпес g анализ пациен...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>297</td>\n",
       "      <td>пз итог недовольный – медсестра рассмотрение м...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>298</td>\n",
       "      <td>стоять поток возмущать » ровно пациентка устра...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                    текст_обращения  label\n",
       "0             0  суть претензия мед работник находиться филиал ...      0\n",
       "1             1  доводить ваш сведение горячий линия обращаться...      1\n",
       "2             2  доводить ваш сведение горячий линия обращаться...      1\n",
       "3             3  доводить ваш сведение горячий линия обращаться...      1\n",
       "4             4  доводить ваш сведение горячий линия обращаться...      1\n",
       "..          ...                                                ...    ...\n",
       "279         294  год приходиться сегодня ребенок очередь обраща...      3\n",
       "280         295  пациентка предъявление полагать претензия родс...      0\n",
       "281         296  пациентка анализ анализ герпес g анализ пациен...      1\n",
       "282         297  пз итог недовольный – медсестра рассмотрение м...      1\n",
       "283         298  стоять поток возмущать » ровно пациентка устра...      0\n",
       "\n",
       "[284 rows x 3 columns]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset = pd.read_csv('augmented-dataset.csv', on_bad_lines='skip')\n",
    "augmented_dataset\n",
    "dataset_no_answers = augmented_dataset\n",
    "dataset_no_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "no_features = 40\n",
    "\n",
    "# dataset_no_answers = pd.read_csv('nn-labeled-dataset.csv') # 4 labels\n",
    "# dataset_no_answers = pd.read_csv('dataset-3-labels.csv') # 3 labels\n",
    "# dataset_no_answers = dataset_no_answers.drop('Unnamed: 0', axis=1) # только для 4 меток, там лишняя колонка\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features, min_df=1, stop_words=russian_stopwords)\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(min_df=1, stop_words=russian_stopwords)\n",
    "X = tfidf_vectorizer.fit_transform(dataset_no_answers[' текст_обращения']).toarray()\n",
    "y = dataset_no_answers['label']\n",
    "\n",
    "len(X)\n",
    "# dataset_no_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8837209302325582"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=10000, random_state=1, max_depth=1000)                                                # 93% best\n",
    "clf = GradientBoostingClassifier(max_features=no_features, n_estimators=10000, learning_rate=0.01, max_depth=100, random_state=1) # 88%\n",
    "# clf = make_pipeline(StandardScaler(), SVC(kernel='sigmoid', gamma='scale'))                           # 67%\n",
    "# clf = MultinomialNB() # 67%\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 1, 3, 1, 0, 1, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 1, 0, 3, 1,\n",
       "       0, 0, 1, 3, 0, 0, 1, 1, 0, 3, 3, 0, 3, 3, 3, 3, 1, 3, 3, 0, 3],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  2  0]\n",
      " [ 0  5  1]\n",
      " [ 0  2 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92        14\n",
      "           1       0.56      0.83      0.67         6\n",
      "           3       0.95      0.91      0.93        23\n",
      "\n",
      "    accuracy                           0.88        43\n",
      "   macro avg       0.84      0.87      0.84        43\n",
      "weighted avg       0.91      0.88      0.89        43\n",
      "\n",
      "0.8837209302325582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# s = pickle.dumps(clf)\n",
    "# exported_model = pickle.loads(s)\n",
    "# exported_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GradientBoostingModel.joblib']"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(clf, 'GradientBoostingModel.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import warnings\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "def make_predict(report_text):\n",
    "    clf = load('GradientBoostingModel.joblib')  \n",
    "\n",
    "    dataset_to_extract_features = pd.read_csv('assets/augmented-dataset.csv', on_bad_lines='skip')\n",
    "    \n",
    "    text_for_features = dataset_to_extract_features[' текст_обращения']\n",
    "    text_for_features = text_for_features.append(pd.Series([report_text]))\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=no_features, min_df=1, stop_words=russian_stopwords)\n",
    "\n",
    "    text_to_predict = tfidf_vectorizer.fit_transform(text_for_features).toarray()\n",
    "\n",
    "    new_text_to_predict = text_to_predict[-1]\n",
    "    new_text_to_predict = [new_text_to_predict]\n",
    "    # print(new_text_to_predict)\n",
    "\n",
    "    # text_to_predict = tfidf_vectorizer.fit_transform([report_text]).toarray()\n",
    "    pr = clf.predict(new_text_to_predict)\n",
    "    return pr\n",
    "\n",
    "# random, 0\n",
    "# sample_text = 'претензия хотеть клиент  –  нужно срочно попадать прием терапевт ничто толком объяснять сотрудник ничто хотеть решать отправлять пациент просить решение вопрос обратный связь'\n",
    "# defined, 1\n",
    "# sample_text = 'доводить ваш сведение горячий линия обращаться обращаться пз 8.30 принимать анализ антитело коронавирус т 1 регистратор большой очередь клиент указывать несоблюдение социальный дистанция регистратор кашель'\n",
    "# defined, 3\n",
    "sample_text = 'доводить ваш сведение горячий линия обращаться пациент пациент обращаться пз краснодар примерно 13 14 час грудной ребенок сдача анализ грубый форма сказать медсестра сдавать анализ необходимо утро т момент обращение сдавать анализ ковид пациент возмущать форма отказ'\n",
    "make_predict(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import warnings\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "def make_predict(report_text):\n",
    "    clf = load('GradientBoostingModel.joblib')  \n",
    "    russian_stopwords = ['и', 'в', 'во', 'не', 'что','он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты',\n",
    "     'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня' 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда',\n",
    "     'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там',\n",
    "     'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб',\n",
    "     'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним',\n",
    "     'здесь', 'этом', 'один', 'почти', 'мой',\n",
    " 'тем',\n",
    " 'чтобы',\n",
    " 'нее',\n",
    " 'сейчас',\n",
    " 'были',\n",
    " 'куда',\n",
    " 'зачем',\n",
    " 'всех',\n",
    " 'никогда',\n",
    " 'можно',\n",
    " 'при',\n",
    " 'наконец',\n",
    " 'два',\n",
    " 'об',\n",
    " 'другой',\n",
    " 'хоть',\n",
    " 'после',\n",
    " 'над',\n",
    " 'больше',\n",
    " 'тот',\n",
    " 'через',\n",
    " 'эти',\n",
    " 'нас',\n",
    " 'про',\n",
    " 'всего',\n",
    " 'них',\n",
    " 'какая',\n",
    " 'много',\n",
    " 'разве',\n",
    " 'три',\n",
    " 'эту',\n",
    " 'моя',\n",
    " 'впрочем',\n",
    " 'хорошо',\n",
    " 'свою',\n",
    " 'этой',\n",
    " 'перед',\n",
    " 'иногда',\n",
    " 'лучше',\n",
    " 'чуть',\n",
    " 'том',\n",
    " 'нельзя',\n",
    " 'такой',\n",
    " 'им',\n",
    " 'более',\n",
    " 'всегда',\n",
    " 'конечно',\n",
    " 'всю',\n",
    " 'между',\n",
    " 'это',\n",
    " 'также',\n",
    " 'данный',\n",
    " 'слово',\n",
    " 'который']\n",
    "    no_features = 40\n",
    "\n",
    "    dataset_to_extract_features = pd.read_csv('assets/augmented-dataset.csv', on_bad_lines='skip')\n",
    "\n",
    "    # with open('assets/stopwords.csv', encoding = 'utf-8') as csvfile:\n",
    "    #     russian_stopwords = list(csv.reader(csvfile))\n",
    "    #     print(russian_stopwords)\n",
    "    \n",
    "\n",
    "    # russian_stopwords = pd.read_csv('assets/stopwords.csv')\n",
    "\n",
    "    text_for_features = dataset_to_extract_features[' текст_обращения']\n",
    "    text_for_features = text_for_features.append(pd.Series([report_text]))\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=no_features, min_df=1, stop_words=russian_stopwords)\n",
    "\n",
    "    text_to_predict = tfidf_vectorizer.fit_transform(text_for_features).toarray()\n",
    "\n",
    "    new_text_to_predict = text_to_predict[-1]\n",
    "    new_text_to_predict = [new_text_to_predict]\n",
    "    # print(new_text_to_predict)\n",
    "\n",
    "    # text_to_predict = tfidf_vectorizer.fit_transform([report_text]).toarray()\n",
    "    pr = clf.predict(new_text_to_predict)\n",
    "    return pr\n",
    "\n",
    "# random, 0\n",
    "# sample_text = 'претензия хотеть клиент  –  нужно срочно попадать прием терапевт ничто толком объяснять сотрудник ничто хотеть решать отправлять пациент просить решение вопрос обратный связь'\n",
    "# defined, 1\n",
    "# sample_text = 'доводить ваш сведение горячий линия обращаться обращаться пз 8.30 принимать анализ антитело коронавирус т 1 регистратор большой очередь клиент указывать несоблюдение социальный дистанция регистратор кашель'\n",
    "# defined, 3\n",
    "sample_text = 'доводить ваш сведение горячий линия обращаться пациент пациент обращаться пз краснодар примерно 13 14 час грудной ребенок сдача анализ грубый форма сказать медсестра сдавать анализ необходимо утро т момент обращение сдавать анализ ковид пациент возмущать форма отказ'\n",
    "make_predict(sample_text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bf4ecafce57d7e80fd4fa0549b97616699ad25c032d6958759ccc36dc147537"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('NewEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
